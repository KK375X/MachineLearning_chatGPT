{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dded4c10",
   "metadata": {},
   "source": [
    "# Softmax 回归\n",
    "Softmax回归是一种用于多类别分类的机器学习方法，通常用于神经网络中的输出层。其主要思想是将模型的原始输出转换成每个类别的概率分布，使得所有类别的概率之和为1。这有助于确定输入数据属于各个类别的概率，最终选择具有最高概率的类别作为预测结果。\n",
    "\n",
    "以下是一个示例，假设我们有一个三类分类问题，其中每个类别用数字标识：\n",
    "\n",
    "1. 原始输出层的结果为：[1.2, 0.9, -0.5]\n",
    "\n",
    "2. 通过Softmax函数进行转换：\n",
    "\n",
    "   - 对于类别1：exp(1.2) / (exp(1.2) + exp(0.9) + exp(-0.5)) ≈ 0.659\n",
    "   - 对于类别2：exp(0.9) / (exp(1.2) + exp(0.9) + exp(-0.5)) ≈ 0.242\n",
    "   - 对于类别3：exp(-0.5) / (exp(1.2) + exp(0.9) + exp(-0.5)) ≈ 0.099\n",
    "\n",
    "3. 最终的分类结果是类别1，因为其对应的概率最高，约为0.659。\n",
    "\n",
    "Softmax回归广泛用于图像分类、自然语言处理等领域的多类别分类任务。\n",
    "\n",
    "# one-hot编码\n",
    "One-hot编码是一种在机器学习中常用的编码方式，用于将分类数据转换为二进制向量，其中只有一个元素为1，其余都为0。每个类别都被编码成一个唯一的二进制向量，以便机器学习算法能够处理这些数据。这通常应用于以下方面：\n",
    "\n",
    "1. **分类问题**：在分类问题中，输入数据可能包含离散的类别信息。通过one-hot编码，将这些类别信息转换为可供模型处理的向量形式。\n",
    "\n",
    "2. **神经网络的输出层**：在神经网络中，输出层通常使用softmax激活函数，其输入要求是一个one-hot编码的向量，以表示类别概率分布。\n",
    "\n",
    "3. **自然语言处理 (NLP)**：在NLP任务中，词汇表中的词汇可以被编码成one-hot向量，用于文本分类、语言建模等任务。\n",
    "\n",
    "4. **图像处理**：在图像处理中，物体或物体部分的检测可以使用one-hot编码的类别标签。\n",
    "\n",
    "5. **推荐系统**：用户和物品的交互数据可以使用one-hot编码表示，以用于推荐系统中的用户-物品匹配。\n",
    "\n",
    "6. **遗传学**：在生物信息学中，DNA序列中的碱基可以使用one-hot编码来表示。\n",
    "\n",
    "示例：考虑一个简单的文本分类任务，其中有三个类别：'汽车'，'食品'，'电子产品'。文本数据的类别标签可以通过one-hot编码表示如下：\n",
    "\n",
    "- '汽车'：[1, 0, 0]\n",
    "- '食品'：[0, 1, 0]\n",
    "- '电子产品'：[0, 0, 1]\n",
    "\n",
    "这样，机器学习模型可以更好地理解和处理类别信息。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85909596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始输出： tensor([ 0.9000,  2.0000, -0.5000])\n",
      "类别概率： tensor([0.2353, 0.7067, 0.0580])\n",
      "预测类别： 1\n",
      "One-hot编码的标签： tensor([0., 1., 0.])\n"
     ]
    }
   ],
   "source": [
    "# one-hot编码与神经网络的输出层相结合的实例\n",
    "# 假设我们有一个神经网络，其输出层有3个神经元，分别对应三个类别。然后，我们需要将这些输出转换为one-hot编码的标签。\n",
    "# 这段代码首先使用softmax函数将原始输出转换为概率分布，然后找到最可能的类别（具有最高概率的类别），最后将其表示为one-hot编码的标签。\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 假设网络的输出层有3个神经元\n",
    "output_layer = torch.tensor([0.9, 2.0, -0.5])  # 这是模型的原始输出\n",
    "\n",
    "# 使用softmax函数将原始输出转换为概率分布\n",
    "probabilities = torch.softmax(output_layer, dim=0)\n",
    "\n",
    "# 找到最可能的类别\n",
    "predicted_class = torch.argmax(probabilities).item()\n",
    "\n",
    "# 创建one-hot编码的标签\n",
    "num_classes = 3\n",
    "one_hot_label = torch.zeros(num_classes)\n",
    "one_hot_label[predicted_class] = 1\n",
    "\n",
    "print(\"原始输出：\", output_layer)\n",
    "print(\"类别概率：\", probabilities)\n",
    "print(\"预测类别：\", predicted_class)\n",
    "print(\"One-hot编码的标签：\", one_hot_label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb29370",
   "metadata": {},
   "source": [
    "# 交叉熵损失\n",
    "交叉熵损失（Cross-Entropy Loss），也称为对数损失（Log Loss），是在分类问题中常用的损失函数，用于度量模型的预测与真实标签之间的差异。以下是一个简单的二分类问题的交叉熵损失的示例：\n",
    "\n",
    "假设我们有一个二分类任务，其中我们要预测一封电子邮件是“垃圾邮件”（类别0）还是“非垃圾邮件”（类别1），并且有一封电子邮件的真实标签是“非垃圾邮件”（类别1）。\n",
    "\n",
    "现在，假设我们的模型产生了以下两个类别的预测概率：\n",
    "\n",
    "- 对于“垃圾邮件”（类别0），预测概率为0.2\n",
    "- 对于“非垃圾邮件”（类别1），预测概率为0.8\n",
    "\n",
    "我们可以使用交叉熵损失来度量模型的预测与真实标签之间的差异：\n",
    "\n",
    "交叉熵损失 = -Σ(真实标签 * log(预测概率))\n",
    "\n",
    "在这个示例中，真实标签是一个长度为2的向量，表示为[0, 1]，因为电子邮件是“非垃圾邮件”（类别1）。因此，交叉熵损失计算如下：\n",
    "\n",
    "交叉熵损失 = -[0 * log(0.2) + 1 * log(0.8)] = -[0 + (-0.2231)] = 0.2231\n",
    "\n",
    "这个0.2231是损失的值，它表示模型的预测与真实标签之间的差异。通常，我们的目标是最小化这个损失，以改善分类模型的性能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5277e897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 给出pytorch实现交叉熵损失的机器学习实例\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 创建一个示例数据集，假设有3个样本，每个样本属于其中一个类别\n",
    "# 这里使用one-hot编码的标签\n",
    "data = torch.tensor([[0.9, 0.1, 0.1], [0.2, 0.7, 0.1], [0.1, 0.2, 0.7]])\n",
    "labels = torch.tensor([0, 1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9b30c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建一个神经网络模型，这里只有一个全连接层作为示例\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(3, 3)  # 输入特征维度为3，输出维度为3，与类别数量相同\n",
    ")\n",
    "\n",
    "# 定义交叉熵损失函数\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 定义优化器\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1bd9ed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 1.0062721967697144\n",
      "Epoch [20/100], Loss: 0.9247240424156189\n",
      "Epoch [30/100], Loss: 0.8545240759849548\n",
      "Epoch [40/100], Loss: 0.7923932671546936\n",
      "Epoch [50/100], Loss: 0.7367215156555176\n",
      "Epoch [60/100], Loss: 0.686549186706543\n",
      "Epoch [70/100], Loss: 0.6411950588226318\n",
      "Epoch [80/100], Loss: 0.6001133322715759\n",
      "Epoch [90/100], Loss: 0.5628390908241272\n",
      "Epoch [100/100], Loss: 0.5289644598960876\n"
     ]
    }
   ],
   "source": [
    "# 训练模型\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()  # 清零梯度\n",
    "\n",
    "    # 前向传播\n",
    "    outputs = model(data)\n",
    "    \n",
    "    # 计算交叉熵损失\n",
    "    loss = criterion(outputs, labels)\n",
    "    \n",
    "    # 反向传播和参数更新\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23f22dd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: 0\n"
     ]
    }
   ],
   "source": [
    "# 使用模型进行预测\n",
    "with torch.no_grad():\n",
    "    new_data = torch.tensor([[0.8, 0.1, 0.1]])\n",
    "    predictions = model(new_data)\n",
    "    predicted_class = torch.argmax(predictions).item()\n",
    "\n",
    "print(f'Predicted class: {predicted_class}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353c7b7f",
   "metadata": {},
   "source": [
    "# 代码分析\n",
    "1. 我们首先创建一个示例的数据集和对应的标签，这里使用了one-hot编码的标签。\n",
    "2. 创建一个简单的神经网络模型，包括一个全连接层，其中输入和输出的维度与类别数量相同。\n",
    "3. 使用`nn.CrossEntropyLoss`定义交叉熵损失函数。\n",
    "4. 使用随机梯度下降（SGD）优化器来更新模型参数。\n",
    "5. 在训练循环中，进行前向传播，计算损失，反向传播，然后更新参数。\n",
    "6. 最后，使用训练好的模型进行预测，输出预测的类别。\n",
    "\n",
    "这个示例演示了如何使用PyTorch实现交叉熵损失，用于多类别分类任务。\n",
    "\n",
    "### optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "这句代码是用于配置和初始化一个随机梯度下降（SGD）优化器的语句。让我解释其中的各个部分：\n",
    "\n",
    "- `optimizer`: 这是一个名为`optimizer`的变量，用于保存优化器的实例，它将负责更新神经网络模型的参数以最小化损失函数。\n",
    "\n",
    "- `optim.SGD`: 这是PyTorch库中的一个模块，表示使用随机梯度下降（Stochastic Gradient Descent）优化算法。SGD是一种常用的优化算法，用于在训练神经网络时更新权重。\n",
    "\n",
    "- `model.parameters()`: 这是将神经网络模型中的参数传递给优化器的部分。`model.parameters()`返回一个包含模型所有可学习参数的迭代器，这些参数将在训练中被优化。\n",
    "\n",
    "- `lr=0.1`: 这是学习率（Learning Rate）的设置。学习率是一个控制每次参数更新步长的超参数。在SGD中，它指定了每次参数更新的大小。0.1是一个常见的初始学习率值，但实际上，学习率的选择可能需要调整和优化以获得最佳的训练结果。\n",
    "\n",
    "所以，这句代码的含义是创建一个SGD优化器，用于更新`model`中的参数，学习率为0.1。这个优化器将在训练过程中使用梯度下降方法来调整模型的权重以最小化损失函数。\n",
    "\n",
    "### predicted_class = torch.argmax(predictions).item()\n",
    "这句代码的含义是用于从模型的预测中找到具有最高概率的类别。让我解释其中的各个部分：\n",
    "\n",
    "- `predictions`: 这是一个包含模型的输出结果的张量。通常，神经网络的输出是一组数字，每个数字对应一个类别的概率或分数。\n",
    "\n",
    "- `torch.argmax(predictions)`: 这是一个PyTorch函数，用于找到`predictions`张量中的最大值所在的索引。换句话说，它返回了预测结果中概率最高的类别的索引。\n",
    "\n",
    "- `.item()`: 这是用于从包含单个元素的张量中获取该元素的数值的方法。因为`torch.argmax(predictions)`返回的是一个张量，我们使用`.item()`来获取该张量中的数值，即最可能的类别的索引。\n",
    "\n",
    "所以，整句代码的目的是找到模型的预测中具有最高概率的类别，并将其索引存储在`predicted_class`变量中，以便后续的分析或输出。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
