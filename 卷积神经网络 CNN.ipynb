{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32f624ce",
   "metadata": {},
   "source": [
    "# 卷积神经网络\n",
    "卷积神经网络（Convolutional Neural Network，CNN）是一种深度学习神经网络，专门用于处理具有网格结构的数据，如图像和视频。它的主要特点是包含卷积层，池化层和全连接层，以有效地捕捉图像中的特征。\n",
    "\n",
    "举个例子，假设你想训练一个CNN来识别猫和狗的图像。CNN的卷积层将扫描图像的小区域，并检测边缘、纹理和颜色等特征。池化层将减小图像的尺寸，同时保留重要信息。最后，全连接层将这些特征映射到猫或狗的分类结果。通过多次迭代和训练，CNN可以逐渐提高其准确性，使其能够准确识别猫和狗的图像。\n",
    "\n",
    "# CNN的三个层\n",
    "卷积神经网络（CNN）通常包含三种主要类型的层，它们分别是：\n",
    "\n",
    "1. 卷积层（Convolutional Layer）：卷积层是CNN的核心组成部分。它使用卷积操作来检测输入数据中的特征，例如边缘、纹理和模式。每个卷积层由多个过滤器（卷积核）组成，这些过滤器在输入数据上滑动，执行卷积运算，生成特征图。卷积操作允许CNN有效地捕捉图像中的空间局部信息。\n",
    "\n",
    "2. 池化层（Pooling Layer）：池化层用于减小特征图的尺寸，同时保留重要信息。最常见的池化操作是最大池化，其中在每个池化窗口内选取最大值，从而减小数据的维度。这有助于降低计算复杂性，提高网络的平移不变性，并减少过拟合。\n",
    "\n",
    "3. 全连接层（Fully Connected Layer）：全连接层是传统神经网络的一部分，它将前一层的所有神经元与当前层的每个神经元相连接。这一层通常出现在CNN的末尾，用于将高级特征映射到最终的分类输出。在图像分类任务中，全连接层通常包括一个输出层，其中每个神经元对应一个可能的类别，以输出类别概率分布。\n",
    "\n",
    "这三种层的组合使CNN能够有效地学习和提取图像中的特征，并在各种计算机视觉任务中表现出色。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12b852cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用pytorch代码实现一个CNN和MNIST数据集的实例，代码中包含损失函数的计算，权重的计算，输出结果语句，输出结果中包含损失函数和权重，在最后创建一个随机数据进行测试。\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F  # 使用 relu() 作为激活函数\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94b42a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一个简单的CNN模型\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(32 * 12 * 12, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = x.view(-1, 32 * 12 * 12)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "608a4ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载MNIST数据集并进行数据预处理\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=False, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e71c6210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化CNN模型、损失函数和优化器\n",
    "cnn = SimpleCNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(cnn.parameters(), lr=0.001, momentum=0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b5533619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.5638719307048234\n",
      "Epoch 2, Loss: 0.18163580343381427\n",
      "Epoch 3, Loss: 0.12984609990311202\n",
      "Epoch 4, Loss: 0.10184404953245892\n",
      "Epoch 5, Loss: 0.08473475530112126\n"
     ]
    }
   ],
   "source": [
    "# 训练模型\n",
    "for epoch in range(5):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = cnn(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {running_loss / (i + 1)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ee19a32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Loss: 0.08473475530112126\n",
      "conv1.weight: tensor([[[[ 2.7359e-02, -1.9373e-02,  1.8455e-02, -4.2632e-03,  3.1818e-02],\n",
      "          [ 1.7663e-01, -2.1356e-01,  1.0963e-02, -1.5305e-01, -1.4808e-01],\n",
      "          [-1.7337e-01,  2.2860e-03,  1.4715e-03, -1.1064e-01,  3.0626e-02],\n",
      "          [-1.5745e-01, -1.1471e-01, -1.2375e-01, -1.6672e-01,  6.6503e-02],\n",
      "          [-4.8797e-02, -1.1807e-01, -1.9582e-01,  1.3672e-01,  2.3478e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 2.4852e-01,  2.9968e-01,  2.1972e-01,  2.3280e-01,  9.4493e-03],\n",
      "          [-1.4184e-01,  8.4172e-02,  1.7060e-01,  2.8415e-02,  1.7135e-01],\n",
      "          [-2.1173e-01, -1.9136e-01, -1.7376e-02, -6.2326e-02,  1.5904e-01],\n",
      "          [ 4.6413e-02, -1.4267e-01, -1.6307e-01,  3.0204e-02, -1.9614e-01],\n",
      "          [-2.3093e-01, -3.0794e-02, -2.3931e-01, -1.5295e-01,  4.1919e-04]]],\n",
      "\n",
      "\n",
      "        [[[-3.2477e-02, -1.4314e-01, -7.5603e-02, -1.9160e-01, -3.1090e-02],\n",
      "          [ 7.7575e-02, -4.7750e-02,  5.7787e-02, -1.3147e-01,  3.8876e-02],\n",
      "          [-1.4272e-01, -1.2595e-01,  4.1229e-02, -2.5527e-01, -7.2870e-02],\n",
      "          [ 1.5687e-03,  1.2031e-01, -3.1003e-02,  5.7139e-02, -1.0438e-01],\n",
      "          [ 2.6570e-01,  2.8019e-01,  1.1444e-01,  1.7324e-01,  1.9429e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 1.0787e-01,  1.6207e-01,  2.8342e-03, -9.5827e-02, -1.5163e-02],\n",
      "          [-1.4803e-01,  1.2774e-01,  1.4617e-01, -9.5258e-02,  7.9181e-02],\n",
      "          [-1.4580e-01,  2.1013e-01,  3.7437e-02, -8.5184e-02, -5.3279e-02],\n",
      "          [-2.4288e-01,  1.6536e-01,  1.9965e-02,  1.0122e-01,  1.8835e-01],\n",
      "          [-1.1215e-01,  5.6430e-03, -1.1512e-01,  3.3404e-02,  7.3977e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 1.0651e-01,  1.2344e-01, -1.8655e-01,  3.5854e-02, -1.9549e-01],\n",
      "          [-1.1672e-01,  6.9098e-02,  1.4233e-01, -1.2534e-01, -9.5761e-02],\n",
      "          [-6.3729e-02,  1.1400e-02,  1.5087e-01, -4.3344e-02, -1.5986e-01],\n",
      "          [-3.4687e-02, -8.9539e-02,  1.9425e-01,  2.2322e-01,  2.2846e-01],\n",
      "          [ 1.3260e-01,  3.8502e-02, -1.7219e-01,  1.2774e-01, -3.8905e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 2.2814e-01,  1.1213e-01,  1.3761e-01,  2.5799e-01,  1.0905e-01],\n",
      "          [-1.1944e-01, -5.5980e-02,  3.1140e-01,  2.1155e-01,  9.6521e-02],\n",
      "          [ 2.0874e-01,  1.3492e-01, -2.6790e-02, -2.0948e-01,  1.2710e-01],\n",
      "          [-5.3618e-02, -2.5380e-01, -2.0983e-01, -1.1772e-01, -1.9610e-01],\n",
      "          [-1.4164e-01, -2.3059e-01, -2.9529e-03, -6.2576e-02, -5.0130e-02]]],\n",
      "\n",
      "\n",
      "        [[[-2.5508e-01, -9.8601e-02,  1.8175e-01,  1.9336e-01, -5.7053e-02],\n",
      "          [-7.7972e-02,  1.5994e-01,  1.7669e-01,  2.2804e-01, -1.5273e-01],\n",
      "          [-1.5181e-01,  2.4334e-01, -1.3405e-02, -1.2443e-01,  4.7065e-02],\n",
      "          [-1.8529e-01,  1.1743e-01,  2.4637e-01, -5.1795e-02, -2.1142e-02],\n",
      "          [ 1.2601e-01,  1.0712e-01,  1.1251e-01, -1.3921e-01, -4.7404e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 2.4126e-01,  9.0297e-02,  6.8462e-03,  1.6424e-04, -3.0952e-02],\n",
      "          [ 1.4591e-01,  2.7948e-01, -7.3949e-02,  1.8998e-01,  2.1911e-01],\n",
      "          [-3.7826e-02,  1.9868e-01,  2.0099e-01,  2.6624e-01,  2.5866e-01],\n",
      "          [-9.8094e-02, -1.8276e-03, -8.9689e-02, -1.4755e-01, -1.7230e-01],\n",
      "          [-2.5754e-01,  4.4328e-02, -9.6440e-02,  2.0377e-02,  1.6909e-01]]],\n",
      "\n",
      "\n",
      "        [[[-9.5834e-02, -1.3036e-01, -2.1393e-01, -1.1505e-01, -1.4692e-01],\n",
      "          [-1.7358e-01, -2.3577e-01,  1.0416e-01,  8.3712e-02,  1.4386e-01],\n",
      "          [ 1.5559e-01,  2.2996e-01,  2.4348e-02, -5.2804e-03,  6.2754e-02],\n",
      "          [ 2.2101e-01,  1.0351e-01, -3.9713e-02,  1.0875e-01, -3.2894e-02],\n",
      "          [-2.5227e-02,  1.0035e-01,  4.6290e-02,  4.7342e-02, -7.2210e-02]]],\n",
      "\n",
      "\n",
      "        [[[-1.7906e-01,  2.0583e-01,  1.3445e-02,  2.7795e-01,  1.2590e-01],\n",
      "          [ 2.6785e-03,  1.2790e-01,  2.1163e-01,  8.3815e-02, -1.4834e-01],\n",
      "          [ 2.0677e-01,  2.4084e-01,  1.7248e-01,  7.8098e-02, -2.1524e-01],\n",
      "          [ 8.5467e-02, -7.8689e-02, -1.6738e-01, -2.3500e-01, -1.8622e-01],\n",
      "          [ 3.6830e-02, -2.2339e-01, -1.6574e-01, -5.2326e-02, -2.1258e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 1.8389e-01,  1.0942e-01,  9.8553e-02,  2.2941e-01,  2.6129e-01],\n",
      "          [ 3.5839e-03,  5.6927e-02, -3.9728e-02,  2.1821e-01,  1.5602e-01],\n",
      "          [ 1.5569e-02, -1.5406e-01, -9.0801e-02, -7.8592e-02, -8.7304e-02],\n",
      "          [-1.7050e-01, -2.3552e-01, -1.0763e-01,  7.4244e-02, -9.5307e-02],\n",
      "          [ 7.5464e-02, -1.1096e-02, -1.0722e-01, -1.0057e-01,  1.3405e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 1.6905e-01, -7.3710e-02, -1.5180e-01, -4.5065e-02, -4.0159e-02],\n",
      "          [-6.6082e-02,  6.0931e-02,  2.4296e-02, -1.7703e-01, -2.4577e-01],\n",
      "          [ 2.2018e-01,  1.2872e-01,  2.5461e-02, -1.9180e-01,  1.4420e-01],\n",
      "          [ 2.5291e-01,  2.4450e-01, -1.4549e-01,  1.0653e-01, -8.2430e-02],\n",
      "          [-1.7646e-01, -7.7820e-02,  6.9545e-02, -3.6463e-02, -9.4993e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 1.2140e-01, -2.0215e-02,  1.4674e-01, -8.7540e-03,  1.1408e-01],\n",
      "          [-7.1916e-02,  5.7230e-02,  8.2005e-02,  4.2074e-02, -8.7470e-03],\n",
      "          [-2.4424e-02,  1.1941e-01, -5.6957e-02, -1.6955e-01,  1.1409e-01],\n",
      "          [-6.1931e-02, -1.4368e-02,  1.5883e-01, -1.7660e-01,  3.1327e-02],\n",
      "          [ 2.4511e-02,  1.9153e-01, -1.3715e-01, -1.0905e-01,  1.1156e-01]]],\n",
      "\n",
      "\n",
      "        [[[-1.9997e-01,  3.0061e-01,  3.0081e-01,  1.7093e-01, -3.9301e-02],\n",
      "          [ 1.6604e-01,  2.6457e-01,  2.2964e-02,  2.5173e-02, -2.1671e-01],\n",
      "          [ 3.2174e-01,  2.9981e-01,  1.6498e-01, -1.7158e-01, -2.9527e-01],\n",
      "          [ 2.5069e-01, -7.0916e-02,  3.7927e-02, -3.3381e-01, -1.9029e-01],\n",
      "          [ 2.4677e-01, -5.4463e-02, -1.7551e-01, -1.5685e-01, -1.0236e-01]]],\n",
      "\n",
      "\n",
      "        [[[-3.4309e-02,  7.9753e-02, -5.3864e-02, -1.2540e-01, -1.0679e-01],\n",
      "          [-1.6710e-01, -2.7362e-02, -6.4189e-02, -1.5830e-01,  5.8152e-03],\n",
      "          [ 2.3049e-01,  1.7036e-01,  6.6411e-02, -1.4706e-01, -2.1939e-01],\n",
      "          [ 1.3918e-01,  1.0422e-01,  5.5531e-02, -5.7230e-02,  2.0436e-03],\n",
      "          [ 1.3003e-01,  8.6359e-02,  2.1442e-01,  1.2532e-01,  1.5349e-01]]],\n",
      "\n",
      "\n",
      "        [[[-3.1806e-02,  2.5282e-01,  3.0036e-01,  5.6380e-02,  3.3767e-01],\n",
      "          [-6.4765e-02,  6.7501e-02,  2.1507e-01,  6.6556e-02,  1.0880e-01],\n",
      "          [-3.1209e-01, -1.8015e-01, -1.6257e-01, -7.7267e-03, -2.6841e-01],\n",
      "          [-2.8735e-01, -3.2350e-01, -2.4304e-01, -2.2325e-01, -6.1954e-02],\n",
      "          [-1.1630e-01, -2.0263e-01, -2.8214e-01, -3.0483e-01, -1.4568e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 1.0137e-01,  8.8376e-02,  2.5568e-01,  1.9030e-01,  5.8377e-02],\n",
      "          [-8.6806e-02,  8.0709e-02,  1.0014e-01,  2.6513e-01,  2.9030e-01],\n",
      "          [ 2.3108e-02, -1.4147e-01, -1.8097e-01, -3.9819e-02,  1.9286e-01],\n",
      "          [-6.3011e-02, -2.5306e-01, -4.2020e-02, -2.4365e-01, -8.3573e-02],\n",
      "          [-1.3947e-01,  1.6405e-02, -2.4125e-01, -7.9376e-02, -2.1219e-01]]],\n",
      "\n",
      "\n",
      "        [[[-2.7450e-01, -1.2957e-01, -1.4118e-02,  3.1282e-01,  1.5994e-01],\n",
      "          [-1.1799e-01,  1.0838e-01,  1.4204e-01,  1.7013e-01,  5.5917e-02],\n",
      "          [-8.9744e-03,  8.7426e-02,  2.5440e-01,  2.3411e-01, -6.5831e-02],\n",
      "          [-1.1722e-01,  1.1490e-01,  1.3398e-01,  2.2952e-01,  9.1940e-02],\n",
      "          [-1.0894e-01,  1.1671e-01,  5.2699e-02,  2.1135e-01,  2.2922e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 3.1702e-02, -3.8966e-02, -2.4238e-01, -2.5664e-01, -2.5822e-02],\n",
      "          [-2.3661e-01,  1.4364e-02, -1.2371e-01,  2.2802e-01,  2.3975e-01],\n",
      "          [-1.2682e-01,  8.1188e-02,  3.0330e-01,  3.1106e-01,  2.7878e-02],\n",
      "          [ 1.7703e-01,  1.4918e-01,  1.1450e-01, -1.7764e-01, -1.7463e-01],\n",
      "          [ 1.3472e-01, -1.4952e-01, -2.5000e-01,  6.7820e-02, -2.1765e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 1.0421e-01,  3.0328e-02,  2.5098e-01,  1.2323e-02,  1.5182e-02],\n",
      "          [-5.7176e-02,  4.1027e-02,  2.4295e-01,  2.3774e-01,  1.9498e-01],\n",
      "          [ 9.5243e-02, -1.5744e-01, -1.7504e-01,  2.2986e-01,  2.0253e-01],\n",
      "          [ 3.3478e-02, -1.3795e-01, -1.7138e-01,  2.2280e-02, -1.9932e-01],\n",
      "          [-1.8697e-01, -2.3393e-01, -1.2362e-01, -9.7251e-02, -2.1237e-01]]],\n",
      "\n",
      "\n",
      "        [[[-2.5593e-01,  6.4919e-02, -2.0647e-01,  6.8879e-02,  1.9301e-01],\n",
      "          [ 7.0878e-02, -3.8432e-02,  6.9012e-02,  1.5405e-01, -1.0405e-01],\n",
      "          [ 2.5098e-01,  1.4913e-01,  2.5080e-01,  1.7521e-01,  1.8929e-01],\n",
      "          [ 1.1098e-01, -1.8778e-01, -1.5040e-01, -1.9630e-01, -2.1419e-01],\n",
      "          [-2.2199e-01,  1.1943e-01, -1.2054e-01,  6.7550e-02, -2.6838e-02]]],\n",
      "\n",
      "\n",
      "        [[[-1.0701e-01,  2.0196e-01,  2.2495e-01,  1.3443e-02, -2.6553e-01],\n",
      "          [-7.3968e-02,  1.3810e-01,  2.3815e-01,  2.8620e-01,  4.6713e-03],\n",
      "          [ 4.4130e-02, -6.7191e-02, -1.5756e-01,  1.2555e-01,  2.6559e-01],\n",
      "          [ 7.2272e-02, -8.5057e-02,  1.0054e-01, -1.1530e-01,  2.8731e-01],\n",
      "          [-2.0901e-01,  9.9948e-02, -8.8932e-02, -2.7148e-02,  1.8660e-01]]],\n",
      "\n",
      "\n",
      "        [[[-1.3166e-01,  1.8493e-01,  2.4020e-01,  1.5314e-01, -2.7720e-01],\n",
      "          [-1.1728e-01,  4.8074e-02,  1.6338e-01, -1.4694e-01, -5.0602e-02],\n",
      "          [-2.7448e-03,  8.8528e-02, -3.1221e-02,  9.5597e-02,  1.9707e-03],\n",
      "          [ 1.5555e-01,  8.7941e-02,  1.0803e-01, -8.4611e-02, -1.2106e-01],\n",
      "          [-1.5169e-01, -2.0832e-01, -1.4069e-01,  1.9041e-01,  2.5007e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 8.5171e-02, -1.0863e-01, -1.1978e-01, -1.0321e-01, -8.6973e-02],\n",
      "          [ 5.6947e-02,  1.0716e-01, -1.0200e-01,  3.4897e-02,  9.5519e-02],\n",
      "          [ 1.8786e-01, -1.3281e-01, -4.3049e-02,  1.3494e-01, -1.6947e-01],\n",
      "          [ 2.1757e-02,  2.3548e-01, -1.8095e-01, -2.0366e-01, -1.4107e-01],\n",
      "          [ 1.8043e-01,  2.6088e-01,  9.0022e-02, -1.9605e-01, -5.0386e-02]]],\n",
      "\n",
      "\n",
      "        [[[-1.3148e-02, -6.9296e-03,  8.1101e-02, -1.7605e-01,  7.8138e-02],\n",
      "          [ 1.5802e-01,  1.9632e-01,  3.4371e-01,  2.9392e-01,  5.3527e-02],\n",
      "          [ 1.2469e-01,  2.5522e-01,  2.6924e-01, -3.8201e-02,  1.2532e-01],\n",
      "          [-2.8002e-01, -2.5401e-01, -2.9975e-02,  9.8978e-02,  1.0996e-01],\n",
      "          [-3.1774e-01, -2.4286e-01, -2.6309e-01, -4.5378e-02, -2.7004e-02]]],\n",
      "\n",
      "\n",
      "        [[[-1.7266e-02,  1.4599e-01,  8.7253e-02,  2.9129e-02, -1.9636e-01],\n",
      "          [ 1.4279e-01,  1.2878e-01,  2.0325e-01, -2.0672e-01, -3.9413e-02],\n",
      "          [-2.5030e-01,  1.4200e-01,  2.3658e-01, -5.7695e-02, -9.5993e-03],\n",
      "          [-1.3372e-01,  1.3341e-01,  2.5866e-01, -2.5868e-02, -1.9870e-01],\n",
      "          [-2.2439e-02,  1.5077e-01,  1.2867e-01,  7.2675e-03, -3.6263e-02]]],\n",
      "\n",
      "\n",
      "        [[[-8.0756e-02, -4.4908e-02,  6.6473e-02,  1.3270e-02, -7.4868e-02],\n",
      "          [ 1.6169e-01,  1.7215e-01,  1.0606e-01, -1.8821e-01,  7.5261e-02],\n",
      "          [ 2.1332e-01, -1.4246e-01, -1.3110e-01, -7.6081e-02, -7.5858e-02],\n",
      "          [ 2.4600e-02, -7.8997e-05, -1.4749e-01,  7.8372e-03,  2.1247e-01],\n",
      "          [-2.1104e-01, -1.7835e-01, -7.2554e-02,  7.7974e-02,  2.6732e-01]]],\n",
      "\n",
      "\n",
      "        [[[-2.0173e-01, -4.5760e-02, -2.3848e-01, -1.5967e-01, -2.8536e-01],\n",
      "          [-1.7787e-01, -2.1719e-01, -1.3760e-01, -2.1761e-01, -1.1700e-01],\n",
      "          [ 6.7431e-02, -1.8625e-01, -9.8293e-02,  1.9408e-02,  1.2586e-01],\n",
      "          [-1.6549e-01,  1.1846e-01,  7.2148e-02, -1.7690e-01,  3.6481e-02],\n",
      "          [-1.9255e-01, -1.7222e-01,  4.7587e-02, -8.9869e-03,  1.4323e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 1.1924e-01,  2.8102e-01,  7.3206e-02,  9.0474e-02, -6.3449e-02],\n",
      "          [-7.4415e-02,  1.8323e-01,  2.1220e-01, -2.2724e-01, -1.5793e-01],\n",
      "          [ 1.6529e-01,  2.3186e-01,  4.0241e-03, -5.7055e-02, -1.9507e-02],\n",
      "          [-7.4267e-02,  3.1876e-01, -1.2699e-01, -1.2787e-01,  2.0487e-03],\n",
      "          [ 2.8167e-01,  1.7109e-01, -7.1030e-02, -6.2499e-02, -1.6947e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 1.9981e-02, -1.7542e-01,  1.3661e-02,  1.6303e-01,  7.4457e-02],\n",
      "          [ 1.4045e-01,  1.3351e-02, -6.0350e-02,  2.1551e-01, -1.5509e-01],\n",
      "          [ 5.2163e-02, -7.2327e-02, -3.2870e-02, -1.6988e-01,  5.7580e-02],\n",
      "          [ 5.0420e-02,  1.6442e-01, -9.0825e-02, -5.0959e-02, -1.3154e-01],\n",
      "          [-3.0272e-02,  3.5057e-03, -4.7844e-02, -2.1625e-01, -2.3754e-02]]],\n",
      "\n",
      "\n",
      "        [[[-7.4322e-02, -9.5586e-02, -6.8114e-02,  1.8654e-01,  9.3245e-03],\n",
      "          [-2.0469e-01, -6.0357e-03,  8.7664e-02,  2.2265e-01, -1.2036e-01],\n",
      "          [-4.4056e-02, -2.2808e-01,  2.2826e-01,  9.8119e-02,  1.1611e-01],\n",
      "          [-1.8141e-01, -1.3634e-01,  6.9437e-02,  2.6349e-01, -1.4331e-01],\n",
      "          [-2.5133e-01, -1.2270e-02,  5.4151e-03, -1.3755e-01,  2.0776e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 1.3642e-01,  1.8532e-02,  1.2099e-01,  8.0167e-02, -2.3351e-01],\n",
      "          [ 1.8381e-01,  1.7438e-01, -1.4906e-01, -6.2246e-02, -1.3753e-01],\n",
      "          [-9.8166e-02,  1.0533e-01, -3.7292e-02, -1.8017e-01, -1.6793e-02],\n",
      "          [-9.8643e-02,  2.2762e-01,  1.5584e-01,  3.7749e-02, -2.2722e-01],\n",
      "          [-1.9810e-02,  2.3288e-01, -3.5020e-02, -1.7536e-01,  4.2872e-02]]]])\n",
      "conv1.bias: tensor([ 0.0502, -0.0816,  0.0358,  0.1937,  0.1351,  0.2078,  0.0987,  0.1121,\n",
      "         0.1518, -0.0475,  0.0733, -0.1581, -0.0626,  0.1445,  0.0988, -0.0017,\n",
      "        -0.1473,  0.0339, -0.1404, -0.0318,  0.2083,  0.0897, -0.0544, -0.1486,\n",
      "         0.1619, -0.0570, -0.0031, -0.0486,  0.1951,  0.0995,  0.1402,  0.0378])\n",
      "fc1.weight: tensor([[-0.0137,  0.0059,  0.0090,  ...,  0.0006, -0.0026, -0.0083],\n",
      "        [ 0.0063,  0.0143, -0.0027,  ...,  0.0151, -0.0071, -0.0094],\n",
      "        [-0.0080,  0.0049,  0.0129,  ..., -0.0134, -0.0155, -0.0005],\n",
      "        ...,\n",
      "        [-0.0077, -0.0050,  0.0050,  ..., -0.0048,  0.0085,  0.0007],\n",
      "        [-0.0017, -0.0014, -0.0101,  ..., -0.0159,  0.0130, -0.0007],\n",
      "        [ 0.0039,  0.0108, -0.0018,  ...,  0.0148,  0.0140,  0.0020]])\n",
      "fc1.bias: tensor([ 1.3194e-02, -1.2199e-02, -3.9992e-03, -3.3643e-03,  8.7506e-03,\n",
      "        -7.4025e-04,  1.2062e-02,  1.1598e-02,  7.7620e-03, -8.0768e-03,\n",
      "        -5.6339e-03,  5.3751e-03,  4.5001e-03, -1.4112e-03,  1.4788e-02,\n",
      "         2.9594e-05, -1.0860e-02,  1.2023e-02, -8.0053e-03,  1.9700e-03,\n",
      "        -4.0310e-03,  8.0142e-03,  1.2813e-02, -8.5787e-03,  2.9315e-03,\n",
      "         4.1047e-03, -7.0339e-03,  4.7784e-03,  4.0311e-04, -9.8771e-03,\n",
      "         1.4426e-02,  1.0941e-02, -8.1353e-03,  8.9619e-03,  1.5290e-02,\n",
      "         3.8304e-03, -5.6018e-03,  1.3260e-02,  1.1802e-02,  1.0792e-02,\n",
      "         1.1535e-02,  1.2137e-02,  7.1865e-03, -1.3431e-02,  4.1250e-03,\n",
      "        -9.3140e-03, -4.6859e-03, -9.6413e-03,  1.4522e-03,  1.3622e-02,\n",
      "        -2.3252e-04, -5.4006e-03,  2.6613e-03, -1.2687e-04, -5.8460e-03,\n",
      "        -1.4551e-02,  8.2899e-04, -2.2430e-03,  8.0336e-03,  7.4638e-03,\n",
      "         6.0829e-05,  4.6841e-03,  3.0434e-03,  3.5464e-03,  4.5822e-03,\n",
      "         7.0446e-03, -9.5512e-03,  1.4280e-02,  1.1061e-02, -9.0685e-05,\n",
      "        -6.0804e-03, -7.0551e-03,  9.0490e-03, -1.3971e-02, -6.5815e-04,\n",
      "        -3.8761e-03,  5.4093e-03, -6.9035e-03,  1.0867e-02,  1.3599e-02,\n",
      "        -1.3577e-02, -3.7510e-03, -1.1989e-02,  6.6773e-04,  9.1113e-03,\n",
      "        -5.8444e-03, -6.0504e-03, -1.1846e-02,  9.8923e-03,  8.7309e-03,\n",
      "         1.2997e-02,  9.6576e-03, -1.0449e-02,  5.3092e-03, -8.0064e-03,\n",
      "        -2.9124e-03, -2.6977e-03,  1.4885e-02,  1.6132e-02,  4.0183e-03,\n",
      "         1.1542e-02, -3.9238e-03, -1.0744e-02,  1.0485e-02,  1.4871e-02,\n",
      "         1.0742e-02, -9.9014e-03,  3.7982e-03, -1.3885e-03,  1.6523e-02,\n",
      "        -6.1754e-03, -2.9275e-03,  3.1183e-03,  7.8174e-03,  1.1682e-02,\n",
      "         7.8349e-03,  3.4852e-03, -1.6089e-03,  2.6760e-03, -9.1911e-03,\n",
      "        -9.9020e-03,  1.7806e-02, -2.6325e-03, -4.4597e-03,  1.4825e-02,\n",
      "        -9.6268e-03,  8.3628e-03,  1.5603e-03])\n",
      "fc2.weight: tensor([[-0.1519,  0.1562,  0.1276,  ...,  0.1227,  0.0143, -0.0345],\n",
      "        [ 0.0084, -0.1251,  0.1180,  ..., -0.0743, -0.0146, -0.1169],\n",
      "        [ 0.0039,  0.2049, -0.0959,  ...,  0.0851,  0.0367,  0.0617],\n",
      "        ...,\n",
      "        [ 0.0589, -0.0171, -0.0582,  ...,  0.0046,  0.0863,  0.0096],\n",
      "        [ 0.0240, -0.1481,  0.1815,  ..., -0.0811, -0.0291, -0.0224],\n",
      "        [ 0.0711,  0.1316, -0.0526,  ..., -0.1276, -0.1289,  0.1224]])\n",
      "fc2.bias: tensor([ 0.0152, -0.0179,  0.0471,  0.0653, -0.0009,  0.0522,  0.0669, -0.0059,\n",
      "         0.0496, -0.0630])\n"
     ]
    }
   ],
   "source": [
    "# 输出损失函数和权重\n",
    "print(f\"Final Loss: {running_loss / (i + 1)}\")\n",
    "for name, param in cnn.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"{name}: {param.data}\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe4e19d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Data Test - Model Output: tensor([[ 0.4571, -2.6542,  0.7330,  0.8477, -0.6852, -0.4386, -0.9702, -0.4646,\n",
      "          3.4684,  1.0092]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 创建随机数据进行测试\n",
    "random_data = torch.rand(1, 1, 28, 28)  # 随机生成一个1x28x28的张量\n",
    "output = cnn(random_data)\n",
    "print(f\"Random Data Test - Model Output: {output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7f8e0f",
   "metadata": {},
   "source": [
    "# 代码分析\n",
    "以下是对上述代码的逐行解释：\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import random\n",
    "```\n",
    "\n",
    "这些是导入必要的PyTorch和TorchVision库，包括用于构建CNN模型、加载数据集以及进行数据预处理的模块。\n",
    "\n",
    "```python\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(32 * 12 * 12, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = x.view(-1, 32 * 12 * 12)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "```\n",
    "\n",
    "这是一个简单的CNN模型定义。`SimpleCNN` 类继承自 `nn.Module`，并包含了卷积层、池化层、全连接层等组件。该模型用于对28x28像素的MNIST手写数字图像进行分类。\n",
    "\n",
    "```python\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "```\n",
    "\n",
    "这一行代码创建了一个数据预处理管道，将图像数据转换为张量并进行标准化，以便在训练期间使用。\n",
    "\n",
    "```python\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "```\n",
    "\n",
    "这里使用TorchVision加载MNIST数据集。`trainset` 包含了训练数据，`trainloader` 用于将数据划分为小批次，每批次包含64个图像。\n",
    "\n",
    "```python\n",
    "cnn = SimpleCNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(cnn.parameters(), lr=0.001, momentum=0.9)\n",
    "```\n",
    "\n",
    "这几行代码初始化了CNN模型、损失函数（交叉熵损失）和优化器（随机梯度下降SGD）。`cnn` 是之前定义的 CNN 模型。\n",
    "\n",
    "```python\n",
    "for epoch in range(5):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = cnn(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {running_loss / (i + 1)}\")\n",
    "```\n",
    "\n",
    "这是训练循环。模型训练了5个周期（epochs）。在每个周期中，它遍历 `trainloader` 中的小批次数据，计算损失并执行反向传播以更新模型权重。`running_loss` 用于跟踪每个周期的平均损失。\n",
    "\n",
    "```python\n",
    "print(f\"Final Loss: {running_loss / (i + 1)}\")\n",
    "```\n",
    "\n",
    "这行代码输出训练结束后的最终平均损失。\n",
    "\n",
    "```python\n",
    "for name, param in cnn.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"{name}: {param.data}\")\n",
    "```\n",
    "\n",
    "这是输出模型的权重参数。\n",
    "\n",
    "```python\n",
    "random_data = torch.rand(1, 1, 28, 28)  # 随机生成一个1x28x28的张量\n",
    "output = cnn(random_data)\n",
    "print(f\"Random Data Test - Model Output: {output}\")\n",
    "```\n",
    "\n",
    "最后，这几行代码创建了一个随机的28x28的张量 `random_data` 并将其输入到已经训练好的 CNN 模型中，输出模型对随机数据的结果。这是一个简单的测试示例。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ac0612",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "059fac5d",
   "metadata": {},
   "source": [
    "### 卷积神经网络\n",
    "卷积神经网络（Convolutional Neural Network，CNN）是一种专门设计用于处理图像和视觉数据的深度学习模型。它的关键特征是卷积层，用于捕获输入数据中的局部特征，并通过多个层级的卷积和池化操作来逐渐提取更高级的特征。这使得CNN在图像处理任务中非常强大，因为它们能够自动学习图像中的特征，而无需手动提取。\n",
    "\n",
    "下面是一个简单的CNN结构的说明以及一个示例：\n",
    "\n",
    "**CNN结构：**\n",
    "1. **卷积层（Convolutional Layer）**：卷积操作用于在输入数据的小窗口上滑动过滤器（卷积核），从而捕获局部特征。每个卷积核会生成一个特征图。\n",
    "2. **激活函数（Activation Function）**：通常在卷积层后面添加激活函数，如ReLU，以引入非线性。\n",
    "3. **池化层（Pooling Layer）**：池化操作用于减小特征图的空间尺寸，同时保留关键信息。常见的池化操作包括最大池化和平均池化。\n",
    "4. **全连接层（Fully Connected Layer）**：在CNN的顶部通常包含一个或多个全连接层，用于将特征映射转化为输出类别的概率分布。\n",
    "\n",
    "**示例：**\n",
    "假设你要训练一个CNN模型来进行图像分类，以识别手写数字的图像。你的CNN模型可以包括多个卷积层，激活函数，池化层和全连接层。训练时，你提供包含手写数字的图像数据集，模型将学会捕获数字的局部特征，如笔画和边缘。\n",
    "\n",
    "例如，当你向这个CNN模型输入一张手写数字的图像，它将通过卷积层和池化层逐渐提取特征，然后通过全连接层输出数字的分类结果，表明输入图像属于0到9中的哪一个数字类别。这使得CNN在数字识别和图像分类任务中非常成功，如MNIST数据集上的手写数字识别。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acadc4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fd62be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义CNN模型\n",
    "class CNN(nn.Module):\n",
    "    # 在 __init__ 方法中，我们初始化了CNN模型的结构，包括一个卷积层（conv1）和两个全连接层（fc1 和 fc2）。\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3)  # 1通道输入图像，32个3x3卷积核\n",
    "        self.fc1 = nn.Linear(32 * 26 * 26, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)  # 10个数字类别\n",
    "\n",
    "    # forward 方法定义了数据在模型中前向传播的方式。数据通过卷积层、激活函数（这里使用ReLU），池化层，全连接层，最终输出分类结果。\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2916d038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据加载和预处理\n",
    "# transform 变量定义了数据预处理的操作，将图像转换为张量并进行标准化。\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "# 使用torchvision.datasets.MNIST加载MNIST手写数字数据集，指定数据集的根目录、训练集标志、数据下载和预处理操作。\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "# 创建一个数据加载器trainloader，用于批量加载训练数据。在这里，每批次包含64张图像，数据会被随机打乱。\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "611df0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化模型、损失函数和优化器\n",
    "# 创建了CNN模型的实例net，定义了损失函数（交叉熵损失）criterion 和优化器（随机梯度下降）optimizer。\n",
    "net = CNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "173ec388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.48383224438597905\n",
      "Epoch 2, Loss: 0.24184392245291775\n",
      "Epoch 3, Loss: 0.18429121055376174\n",
      "Epoch 4, Loss: 0.1452784146557548\n",
      "Epoch 5, Loss: 0.12127579923774769\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# 训练模型\n",
    "for epoch in range(5):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # 获取批次数据（inputs 和 labels）。\n",
    "        inputs, labels = data\n",
    "        # 将优化器梯度置零（optimizer.zero_grad()）。\n",
    "        optimizer.zero_grad()\n",
    "        # 将数据传递给模型，获取模型的预测结果（outputs）。\n",
    "        outputs = net(inputs)\n",
    "        # 计算损失值（loss）。\n",
    "        loss = criterion(outputs, labels)\n",
    "        # 反向传播梯度并执行优化（loss.backward() 和 optimizer.step()）。\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # 累积损失值以进行训练过程中的监控。\n",
    "        running_loss += loss.item()\n",
    "    print(f'Epoch {epoch + 1}, Loss: {running_loss / len(trainloader)}')\n",
    "\n",
    "print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b51347c",
   "metadata": {},
   "source": [
    "### 在__init__函数中的代码有什么作用\n",
    "在上述代码中，`__init__` 函数是CNN模型类的构造函数，它在模型被实例化时被调用。`__init__` 函数的主要作用是初始化模型的结构和参数。以下是具体的解释：\n",
    "\n",
    "1. `super(CNN, self).__init__()`：这一行调用父类的构造函数，确保正确初始化继承自`nn.Module`的基类。\n",
    "\n",
    "2. `self.conv1 = nn.Conv2d(1, 32, 3)`：这一行创建了一个卷积层 `conv1`。这个卷积层有以下参数：\n",
    "   - `1`：输入通道数，表示输入图像的通道数。在MNIST数据集中，图像是单通道的（灰度图像），因此为1。\n",
    "   - `32`：输出通道数，表示卷积核的数量，决定了卷积层的输出特征图数目。\n",
    "   - `3`：卷积核的大小，这里是3x3的卷积核。\n",
    "\n",
    "3. `self.fc1 = nn.Linear(32 * 26 * 26, 128)`：这一行创建了一个全连接层 `fc1`。全连接层将卷积层的输出展平，并连接到具有128个神经元的隐藏层。\n",
    "\n",
    "4. `self.fc2 = nn.Linear(128, 10)`：这一行创建了另一个全连接层 `fc2`，输出10个神经元，对应于手写数字的10个可能类别。\n",
    "\n",
    "在 `__init__` 函数中，我们定义了CNN模型的结构，包括一个卷积层和两个全连接层。这些层的参数会在模型初始化时被自动分配，然后在模型的前向传播中使用。这个初始化步骤非常重要，因为它定义了模型的基本架构。在训练过程中，模型将学习适应数据的权重和偏置，以便在任务中表现良好。\n",
    "\n",
    "### 在forward函数中的代码有什么作用\n",
    "在上述代码中，`forward` 函数定义了数据在模型中前向传播的方式。这个函数的主要作用是描述了数据从输入到输出的处理过程，包括如何经过卷积层、激活函数、池化层以及全连接层等模块。\n",
    "\n",
    "具体来说，以下是 `forward` 函数中代码的作用：\n",
    "\n",
    "1. `x = torch.relu(self.conv1(x))`：这一行将输入 `x` 传递给卷积层 `self.conv1`，并使用ReLU激活函数对卷积层的输出进行非线性转换。这一步通过卷积和激活捕获了输入图像的特征。\n",
    "\n",
    "2. `x = x.view(x.size(0), -1)`：这一行将卷积层的输出展平，以便将其传递给全连接层。`x.size(0)` 表示批次大小，`-1` 表示将其余维度展平。这是因为全连接层期望扁平化的输入。\n",
    "\n",
    "3. `x = torch.relu(self.fc1(x))`：这一行将扁平化后的数据传递给第一个全连接层 `self.fc1`，并再次应用ReLU激活函数。\n",
    "\n",
    "4. `x = self.fc2(x)`：最后，这一行将数据传递给第二个全连接层 `self.fc2`，不使用激活函数。这一层输出一个长度为10的向量，代表了10个数字类别的得分（例如，0到9）。\n",
    "\n",
    "通过 `forward` 函数，我们定义了整个CNN模型的前向传播过程，将输入数据经过卷积、激活、池化、全连接等操作，最终产生了分类结果。这个函数是PyTorch中模型定义的核心部分，它规定了数据如何通过网络传递，允许PyTorch自动构建反向传播图以便训练模型。\n",
    "\n",
    "### 解释数据加载和预处理部分的三行代码\n",
    "数据加载和预处理部分的三行代码主要负责加载训练数据并进行数据预处理。以下是这三行代码的解释：\n",
    "\n",
    "1. `transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])`：\n",
    "   - 这一行代码创建了一个数据预处理的管道（pipeline）。\n",
    "   - `transforms.Compose` 允许将多个数据预处理操作组合成一个序列。\n",
    "   - `transforms.ToTensor()` 将图像数据从PIL图像格式转换为PyTorch张量，同时缩放像素值到[0, 1]范围。\n",
    "   - `transforms.Normalize((0.5,), (0.5,))` 对每个通道的张量数据进行均值和标准差的归一化操作，将像素值归一化到大致[-1, 1]的范围。这是为了帮助模型更好地训练。\n",
    "\n",
    "2. `trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)`：\n",
    "   - 这一行代码创建了一个`trainset`对象，用于加载MNIST数据集的训练集部分。\n",
    "   - `root='./data'` 指定了数据集下载后存放的目录。\n",
    "   - `train=True` 表明加载训练集数据。\n",
    "   - `download=True` 表示如果数据不存在，会自动下载数据。\n",
    "   - `transform=transform` 将数据预处理管道应用于加载的图像数据。\n",
    "\n",
    "3. `trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)`：\n",
    "   - 这一行创建了一个数据加载器 `trainloader`，用于以批量的方式加载训练数据。\n",
    "   - `trainset` 是包含了MNIST训练数据集的数据集对象。\n",
    "   - `batch_size=64` 指定了每个批次包含的样本数，这里是64。\n",
    "   - `shuffle=True` 表示在每个轮次开始时随机打乱数据，以确保模型不会受到数据的顺序影响。\n",
    "\n",
    "这些代码片段的作用是将MNIST训练数据集加载到PyTorch张量中，并进行必要的预处理，以便在训练期间使用。数据加载和预处理是机器学习中非常重要的步骤，确保数据对模型的训练产生正面影响。\n",
    "\n",
    "### 如果已经下载了数据集，但是download的值仍为true，数据集会被重复下载吗\n",
    "在PyTorch的`torchvision.datasets`中，如果你已经手动下载了数据集，并将`download`参数设置为`True`，数据集将不会被重复下载。PyTorch会检查数据集存储目录中是否已存在相应的数据文件，如果已存在，它将跳过下载并直接使用已有的数据。\n",
    "\n",
    "这种设置非常有用，因为它允许你确保数据集的完整性，同时避免不必要的重复下载，节省带宽和时间。如果你已手动下载了数据集，并设置了`download`为`True`，PyTorch将仅在必要时进行下载，否则会直接使用现有数据。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
